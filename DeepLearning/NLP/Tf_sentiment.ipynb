{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.28M/1.28M [00:00<00:00, 1.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading test.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 494k/494k [00:00<00:00, 713kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# must set the kaggle api json file to the path\n",
    "# AND MUST TAKE PART TO THE SPECIFIC COMPETITION\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# api.competition_download_file('sentiment-analysis-on-movie-reviews', f'train.tsv.zip', path='./')\n",
    "\n",
    "for file in ['train.tsv', 'test.tsv']:\n",
    "    api.competition_download_file('sentiment-analysis-on-movie-reviews', f'{file}.zip', path='./')\n",
    "\n",
    "    with zipfile.ZipFile(f'{file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "\n",
    "    os.remove(f'{file}.zip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "   PhraseId  SentenceId                                             Phrase  \\\n0         1           1  A series of escapades demonstrating the adage ...   \n1         2           1  A series of escapades demonstrating the adage ...   \n2         3           1                                           A series   \n3         4           1                                                  A   \n4         5           1                                             series   \n\n   Sentiment  \n0          1  \n1          2  \n2          2  \n3          2  \n4          2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='Sentiment'>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts().plot(kind='bar')\n",
    "# labels: negative, somewhat negative, neutral, somewhat neutral, positive\n",
    "# Let's check the distribution of sentiment classes across our data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(156060, 512)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be tokenizing this text to create two input tensors; our input IDs, and attention mask.\n",
    "# We will contain our tensors within two numpy arrays, which will be of dimensions len(df) * 512 - the 512 is the sequence length of our tokenized sequences for BERT, and len(df) the number of samples in our dataset.\n",
    "\n",
    "seq_len = 512\n",
    "num_samples = len(df)\n",
    "num_samples, seq_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Now we can begin tokenizing with a BertTokenizer, like so:\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# tokenize - this time returning Numpy tensors\n",
    "tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding='max_length', add_special_tokens=True,\n",
    "                   return_tensors='np')\n",
    "\n",
    "# np= numpy arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['attention_mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  101,   138,  1326, ...,     0,     0,     0],\n       [  101,   138,  1326, ...,     0,     0,     0],\n       [  101,   138,  1326, ...,     0,     0,     0],\n       ...,\n       [  101, 13936, 25265, ...,     0,     0,     0],\n       [  101, 13936, 25265, ...,     0,     0,     0],\n       [  101, 15107,  1103, ...,     0,     0,     0]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(['A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n  'A series of escapades demonstrating the adage that what is good for the goose',\n  'A series',\n  'A',\n  'series',\n  'of escapades demonstrating the adage that what is good for the goose',\n  'of',\n  'escapades demonstrating the adage that what is good for the goose',\n  'escapades',\n  'demonstrating the adage that what is good for the goose',\n  'demonstrating the adage',\n  'demonstrating',\n  'the adage',\n  'the',\n  'adage',\n  'that what is good for the goose',\n  'that',\n  'what is good for the goose',\n  'what',\n  'is good for the goose',\n  'is',\n  'good for the goose',\n  'good',\n  'for the goose',\n  'for',\n  'the goose',\n  'goose',\n  'is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n  'is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story',\n  'is also',\n  'also',\n  'good for the gander , some of which occasionally amuses but none of which amounts to much of a story',\n  'for the gander , some of which occasionally amuses but none of which amounts to much of a story',\n  'the gander , some of which occasionally amuses but none of which amounts to much of a story',\n  'the gander ,',\n  'the gander',\n  'gander',\n  ',',\n  'some of which occasionally amuses but none of which amounts to much of a story',\n  'some of which',\n  'some',\n  'of which',\n  'which',\n  'occasionally amuses but none of which amounts to much of a story',\n  'occasionally',\n  'amuses but none of which amounts to much of a story',\n  'amuses',\n  'but none of which amounts to much of a story',\n  'but',\n  'none of which amounts to much of a story',\n  'none',\n  'of which amounts to much of a story',\n  'which amounts to much of a story',\n  'amounts to much of a story',\n  'amounts',\n  'to much of a story',\n  'to',\n  'much of a story',\n  'much',\n  'of a story',\n  'a story',\n  'story',\n  '.',\n  'This quiet , introspective and entertaining independent is worth seeking .',\n  'This quiet , introspective and entertaining independent',\n  'This',\n  'quiet , introspective and entertaining independent',\n  'quiet , introspective and entertaining',\n  'quiet',\n  ', introspective and entertaining',\n  'introspective and entertaining',\n  'introspective and',\n  'introspective',\n  'and',\n  'entertaining',\n  'independent',\n  'is worth seeking .',\n  'is worth seeking',\n  'is worth',\n  'worth',\n  'seeking',\n  \"Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .\",\n  \"Even fans of Ismail Merchant 's work\",\n  'Even fans',\n  'Even',\n  'fans',\n  \"of Ismail Merchant 's work\",\n  \"Ismail Merchant 's work\",\n  \"Ismail Merchant 's\",\n  'Ismail',\n  \"Merchant 's\",\n  'Merchant',\n  \"'s\",\n  'work',\n  ', I suspect , would have a hard time sitting through this one .',\n  ', I suspect ,',\n  'I suspect ,',\n  'I suspect',\n  'I',\n  'suspect',\n  'would have a hard time sitting through this one .',\n  'would have a hard time sitting through this one',\n  'would',\n  'have a hard time sitting through this one',\n  'have',\n  'a hard time sitting through this one',\n  'a hard time',\n  'hard time',\n  'hard',\n  'time',\n  'sitting through this one',\n  'sitting',\n  'through this one',\n  'through',\n  'this one',\n  'one',\n  'A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .',\n  'A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera',\n  'A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder',\n  'A positively thrilling combination',\n  'positively thrilling combination',\n  'positively',\n  'thrilling combination',\n  'thrilling',\n  'combination',\n  'of ethnography and all the intrigue , betrayal , deceit and murder',\n  'ethnography and all the intrigue , betrayal , deceit and murder',\n  'ethnography and',\n  'ethnography',\n  'all the intrigue , betrayal , deceit and murder',\n  'all',\n  'the intrigue , betrayal , deceit and murder',\n  'intrigue , betrayal , deceit and murder',\n  'intrigue',\n  ', betrayal , deceit and murder',\n  'betrayal , deceit and murder',\n  'betrayal',\n  ', deceit and murder',\n  'deceit and murder',\n  'deceit and',\n  'deceit',\n  'murder',\n  'of a Shakespearean tragedy or a juicy soap opera',\n  'a Shakespearean tragedy or a juicy soap opera',\n  'a Shakespearean tragedy or',\n  'a Shakespearean tragedy',\n  'Shakespearean tragedy',\n  'Shakespearean',\n  'tragedy',\n  'or',\n  'a juicy soap opera',\n  'juicy soap opera',\n  'juicy',\n  'soap opera',\n  'soap',\n  'opera',\n  'Aggressive self-glorification and a manipulative whitewash .',\n  'Aggressive self-glorification and a manipulative whitewash',\n  'Aggressive',\n  'self-glorification and a manipulative whitewash',\n  'self-glorification and',\n  'self-glorification',\n  'a manipulative whitewash',\n  'manipulative whitewash',\n  'manipulative',\n  'whitewash',\n  'A comedy-drama of nearly epic proportions rooted in a sincere performance by the title character undergoing midlife crisis .',\n  'A comedy-drama of nearly epic proportions',\n  'A comedy-drama',\n  'comedy-drama',\n  'of nearly epic proportions',\n  'nearly epic proportions',\n  'nearly epic',\n  'nearly',\n  'epic',\n  'proportions',\n  'rooted in a sincere performance by the title character undergoing midlife crisis .',\n  'rooted in a sincere performance by the title character undergoing midlife crisis',\n  'rooted in a sincere performance',\n  'rooted',\n  'in a sincere performance',\n  'in',\n  'a sincere performance',\n  'sincere performance',\n  'sincere',\n  'performance',\n  'by the title character undergoing midlife crisis',\n  'by',\n  'the title character undergoing midlife crisis',\n  'the title character',\n  'title character',\n  'title',\n  'character',\n  'undergoing midlife crisis',\n  'undergoing',\n  'midlife crisis',\n  'midlife',\n  'crisis',\n  'Narratively , Trouble Every Day is a plodding mess .',\n  'Narratively',\n  ', Trouble Every Day is a plodding mess .',\n  'Trouble Every Day is a plodding mess .',\n  'Trouble Every Day',\n  'Trouble',\n  'Every Day',\n  'Every',\n  'Day',\n  'is a plodding mess .',\n  'is a plodding mess',\n  'a plodding mess',\n  'plodding mess',\n  'plodding',\n  'mess',\n  \"The Importance of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'The Importance',\n  'Importance',\n  \"of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  \"Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'Being',\n  \"Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'Earnest ,',\n  'Earnest',\n  \"so thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'so',\n  \"thick with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'thick',\n  \"with wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'with',\n  \"wit it plays like a reading from Bartlett 's Familiar Quotations\",\n  'wit',\n  \"it plays like a reading from Bartlett 's Familiar Quotations\",\n  'it',\n  \"plays like a reading from Bartlett 's Familiar Quotations\",\n  'plays',\n  \"like a reading from Bartlett 's Familiar Quotations\",\n  'like',\n  \"a reading from Bartlett 's Familiar Quotations\",\n  'a reading',\n  'reading',\n  \"from Bartlett 's Familiar Quotations\",\n  'from',\n  \"Bartlett 's Familiar Quotations\",\n  \"Bartlett 's\",\n  'Bartlett',\n  'Familiar Quotations',\n  'Familiar',\n  'Quotations',\n  \"But it does n't leave you with much .\",\n  \"it does n't leave you with much .\",\n  \"does n't leave you with much .\",\n  \"does n't leave you with much\",\n  \"does n't\",\n  'does',\n  \"n't\",\n  'leave you with much',\n  'leave you',\n  'leave',\n  'you',\n  'with much',\n  'You could hate it for the same reason .',\n  'could hate it for the same reason .',\n  'could hate it for the same reason',\n  'could',\n  'hate it for the same reason',\n  'hate it',\n  'hate',\n  'for the same reason',\n  'the same reason',\n  'same reason',\n  'same',\n  'reason',\n  \"There 's little to recommend Snow Dogs , unless one considers cliched dialogue and perverse escapism a source of high hilarity .\",\n  'There',\n  \"'s little to recommend Snow Dogs , unless one considers cliched dialogue and perverse escapism a source of high hilarity .\",\n  \"'s little to recommend Snow Dogs , unless one considers cliched dialogue and perverse escapism a source of high hilarity\",\n  \"'s little to recommend Snow Dogs ,\",\n  \"'s little to recommend Snow Dogs\",\n  'little to recommend Snow Dogs',\n  'little',\n  'to recommend Snow Dogs',\n  'recommend Snow Dogs',\n  'recommend',\n  'Snow Dogs',\n  'Snow',\n  'Dogs',\n  'unless one considers cliched dialogue and perverse escapism a source of high hilarity',\n  'unless',\n  'one considers cliched dialogue and perverse escapism a source of high hilarity',\n  'considers cliched dialogue and perverse escapism a source of high hilarity',\n  'considers',\n  'cliched dialogue and perverse escapism a source of high hilarity',\n  'cliched dialogue and perverse escapism',\n  'cliched dialogue and',\n  'cliched dialogue',\n  'cliched',\n  'dialogue',\n  'perverse escapism',\n  'perverse',\n  'escapism',\n  'a source of high hilarity',\n  'a source',\n  'source',\n  'of high hilarity',\n  'high hilarity',\n  'high',\n  'hilarity',\n  \"Kung Pow is Oedekerk 's realization of his childhood dream to be in a martial-arts flick , and proves that sometimes the dreams of youth should remain just that .\",\n  'Kung Pow',\n  'Kung',\n  'Pow',\n  \"is Oedekerk 's realization of his childhood dream to be in a martial-arts flick , and proves that sometimes the dreams of youth should remain just that .\",\n  \"is Oedekerk 's realization of his childhood dream to be in a martial-arts flick , and proves that sometimes the dreams of youth should remain just that\",\n  \"is Oedekerk 's realization of his childhood dream to be in a martial-arts flick , and\",\n  \"is Oedekerk 's realization of his childhood dream to be in a martial-arts flick ,\",\n  \"is Oedekerk 's realization of his childhood dream to be in a martial-arts flick\",\n  \"Oedekerk 's realization of his childhood dream to be in a martial-arts flick\",\n  \"Oedekerk 's realization of his childhood dream\",\n  \"Oedekerk 's realization\",\n  \"Oedekerk 's\",\n  'Oedekerk',\n  'realization',\n  'of his childhood dream',\n  'his childhood dream',\n  'his',\n  'childhood dream',\n  'childhood',\n  'dream',\n  'to be in a martial-arts flick',\n  'be in a martial-arts flick',\n  'be',\n  'in a martial-arts flick',\n  'a martial-arts flick',\n  'martial-arts flick',\n  'martial-arts',\n  'flick',\n  'proves that sometimes the dreams of youth should remain just that',\n  'proves',\n  'that sometimes the dreams of youth should remain just that',\n  'sometimes the dreams of youth should remain just that',\n  'sometimes',\n  'the dreams of youth should remain just that',\n  'the dreams of youth',\n  'the dreams',\n  'dreams',\n  'of youth',\n  'youth',\n  'should remain just that',\n  'should',\n  'remain just that',\n  'remain',\n  'just that',\n  'just',\n  'The performances are an absolute joy .',\n  'The performances',\n  'performances',\n  'are an absolute joy .',\n  'are an absolute joy',\n  'are',\n  'an absolute joy',\n  'an',\n  'absolute joy',\n  'absolute',\n  'joy',\n  'Fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .',\n  'Fresnadillo',\n  'has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .',\n  'has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'has',\n  'something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'something',\n  'serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'serious',\n  'to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'say',\n  'about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'about',\n  'the ways in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'the ways',\n  'ways',\n  'in which extravagant chance can distort our perspective and throw us off the path of good sense',\n  'in which',\n  'extravagant chance can distort our perspective and throw us off the path of good sense',\n  'extravagant chance',\n  'extravagant',\n  'chance',\n  'can distort our perspective and throw us off the path of good sense',\n  'can',\n  'distort our perspective and throw us off the path of good sense',\n  'distort our perspective and',\n  'distort our perspective',\n  'distort',\n  'our perspective',\n  'our',\n  'perspective',\n  'throw us off the path of good sense',\n  'throw us',\n  'throw',\n  'us',\n  'off the path of good sense',\n  'off',\n  'the path of good sense',\n  'the path',\n  'path',\n  'of good sense',\n  'good sense',\n  'sense',\n  'I still like Moonlight Mile , better judgment be damned .',\n  'still like Moonlight Mile , better judgment be damned .',\n  'still',\n  'like Moonlight Mile , better judgment be damned .',\n  'like Moonlight Mile , better judgment be damned',\n  'Moonlight Mile , better judgment be damned',\n  'Moonlight Mile , better judgment',\n  'Moonlight',\n  'Mile , better judgment',\n  'Mile',\n  ', better judgment',\n  'better judgment',\n  'better',\n  'judgment',\n  'be damned',\n  'damned',\n  'A welcome relief from baseball movies that try too hard to be mythic , this one is a sweet and modest and ultimately winning story .',\n  'A welcome relief from baseball movies that try too hard to be mythic',\n  'A welcome relief',\n  'welcome relief',\n  'welcome',\n  'relief',\n  'from baseball movies that try too hard to be mythic',\n  'baseball movies that try too hard to be mythic',\n  'baseball movies',\n  'baseball',\n  'movies',\n  'that try too hard to be mythic',\n  'try too hard to be mythic',\n  'try',\n  'too hard to be mythic',\n  'too',\n  'hard to be mythic',\n  'to be mythic',\n  'be mythic',\n  'mythic',\n  ', this one is a sweet and modest and ultimately winning story .',\n  'this one is a sweet and modest and ultimately winning story .',\n  'is a sweet and modest and ultimately winning story .',\n  'is a sweet and modest and ultimately winning story',\n  'a sweet and modest and ultimately winning story',\n  'a sweet and modest and',\n  'a sweet and modest',\n  'sweet and modest',\n  'sweet and',\n  'sweet',\n  'modest',\n  'ultimately winning story',\n  'ultimately',\n  'winning story',\n  'winning',\n  'a bilingual charmer , just like the woman who inspired it',\n  'a bilingual charmer ,',\n  'a bilingual charmer',\n  'bilingual charmer',\n  'bilingual',\n  'charmer',\n  'just like the woman who inspired it',\n  'like the woman who inspired it',\n  'the woman who inspired it',\n  'the woman',\n  'woman',\n  'who inspired it',\n  'who',\n  'inspired it',\n  'inspired',\n  \"Like a less dizzily gorgeous companion to Mr. Wong 's In the Mood for Love -- very much a Hong Kong movie despite its mainland setting .\",\n  'Like a less dizzily gorgeous companion to Mr.',\n  'a less dizzily gorgeous companion to Mr.',\n  'a less dizzily gorgeous companion',\n  'less dizzily gorgeous companion',\n  'less dizzily gorgeous',\n  'less',\n  'dizzily gorgeous',\n  'dizzily',\n  'gorgeous',\n  'companion',\n  'to Mr.',\n  'Mr.',\n  \"Wong 's In the Mood for Love -- very much a Hong Kong movie despite its mainland setting .\",\n  'Wong',\n  \"'s In the Mood for Love -- very much a Hong Kong movie despite its mainland setting .\",\n  \"'s In the Mood for Love -- very much a Hong Kong movie despite its mainland setting\",\n  \"'s In the Mood for Love -- very much a Hong Kong movie\",\n  'In the Mood for Love -- very much a Hong Kong movie',\n  'the Mood for Love -- very much a Hong Kong movie',\n  'the Mood',\n  'Mood',\n  'for Love -- very much a Hong Kong movie',\n  'Love -- very much a Hong Kong movie',\n  'Love -- very much',\n  'Love --',\n  'Love',\n  '--',\n  'very much',\n  'very',\n  'a Hong Kong movie',\n  'Hong Kong movie',\n  'Hong',\n  'Kong movie',\n  'Kong',\n  'movie',\n  'despite its mainland setting',\n  'despite',\n  'its mainland setting',\n  'its',\n  'mainland setting',\n  'mainland',\n  'setting',\n  'As inept as big-screen remakes of The Avengers and The Wild Wild West .',\n  'As inept as big-screen remakes of The Avengers and The Wild Wild West',\n  'As inept',\n  'As',\n  'inept',\n  'as big-screen remakes of The Avengers and The Wild Wild West',\n  'big-screen remakes of The Avengers and The Wild Wild West',\n  'big-screen remakes',\n  'big-screen',\n  'remakes',\n  'of The Avengers and The Wild Wild West',\n  'The Avengers and The Wild Wild West',\n  'The Avengers and',\n  'The Avengers',\n  'Avengers',\n  'The Wild Wild West',\n  'Wild Wild West',\n  'Wild',\n  'Wild West',\n  'West',\n  \"It 's everything you 'd expect -- but nothing more .\",\n  \"'s everything you 'd expect -- but nothing more .\",\n  \"'s everything you 'd expect -- but nothing more\",\n  \"everything you 'd expect -- but nothing more\",\n  'everything',\n  \"you 'd expect -- but nothing more\",\n  \"'d expect -- but nothing more\",\n  \"'d\",\n  'expect -- but nothing more',\n  'expect -- but nothing',\n  'expect --',\n  'expect',\n  'but nothing',\n  'nothing',\n  'more',\n  'Best indie of the year , so far .',\n  'Best',\n  'indie of the year , so far .',\n  'indie of the year , so far',\n  'indie of the year ,',\n  'indie of the year',\n  'indie',\n  'of the year',\n  'the year',\n  'year',\n  'so far',\n  'far',\n  \"Hatfield and Hicks make the oddest of couples , and in this sense the movie becomes a study of the gambles of the publishing world , offering a case study that exists apart from all the movie 's political ramifications .\",\n  \"Hatfield and Hicks make the oddest of couples , and in this sense the movie becomes a study of the gambles of the publishing world , offering a case study that exists apart from all the movie 's political ramifications\",\n  'Hatfield and Hicks make the oddest of couples , and',\n  'Hatfield and Hicks make the oddest of couples ,',\n  'Hatfield and Hicks make the oddest of couples',\n  'Hatfield and Hicks',\n  'Hatfield and',\n  'Hatfield',\n  'Hicks',\n  'make the oddest of couples',\n  'make',\n  'the oddest of couples',\n  'the oddest',\n  'oddest',\n  'of couples',\n  'couples',\n  \"in this sense the movie becomes a study of the gambles of the publishing world , offering a case study that exists apart from all the movie 's political ramifications\",\n  'in this sense',\n  'this sense',\n  \"the movie becomes a study of the gambles of the publishing world , offering a case study that exists apart from all the movie 's political ramifications\",\n  'the movie',\n  \"becomes a study of the gambles of the publishing world , offering a case study that exists apart from all the movie 's political ramifications\",\n  'becomes a study of the gambles of the publishing world ,',\n  'becomes a study of the gambles of the publishing world',\n  'becomes',\n  'a study of the gambles of the publishing world',\n  'a study',\n  'study',\n  'of the gambles of the publishing world',\n  'the gambles of the publishing world',\n  'the gambles',\n  'gambles',\n  'of the publishing world',\n  'the publishing world',\n  'publishing world',\n  'publishing',\n  'world',\n  \"offering a case study that exists apart from all the movie 's political ramifications\",\n  'offering',\n  \"a case study that exists apart from all the movie 's political ramifications\",\n  'a case study',\n  'case study',\n  'case',\n  \"that exists apart from all the movie 's political ramifications\",\n  \"exists apart from all the movie 's political ramifications\",\n  'exists apart',\n  'exists',\n  'apart',\n  \"from all the movie 's political ramifications\",\n  \"all the movie 's political ramifications\",\n  \"the movie 's political ramifications\",\n  \"the movie 's\",\n  \"movie 's\",\n  'political ramifications',\n  'political',\n  'ramifications',\n  \"It 's like going to a house party and watching the host defend himself against a frothing ex-girlfriend .\",\n  \"'s like going to a house party and watching the host defend himself against a frothing ex-girlfriend .\",\n  \"'s like going to a house party and watching the host defend himself against a frothing ex-girlfriend\",\n  'like going to a house party and watching the host defend himself against a frothing ex-girlfriend',\n  'going to a house party and watching the host defend himself against a frothing ex-girlfriend',\n  'going to a house party and',\n  'going to a house party',\n  'going',\n  'to a house party',\n  'a house party',\n  'house party',\n  'house',\n  'party',\n  'watching the host defend himself against a frothing ex-girlfriend',\n  'watching',\n  'the host defend himself against a frothing ex-girlfriend',\n  'the host',\n  'host',\n  'defend himself against a frothing ex-girlfriend',\n  'defend himself',\n  'defend',\n  'himself',\n  'against a frothing ex-girlfriend',\n  'against',\n  'a frothing ex-girlfriend',\n  'frothing ex-girlfriend',\n  'frothing',\n  'ex-girlfriend',\n  \"That the Chuck Norris `` grenade gag '' occurs about 7 times during Windtalkers is a good indication of how serious-minded the film is .\",\n  \"That the Chuck Norris `` grenade gag ''\",\n  \"the Chuck Norris `` grenade gag ''\",\n  \"Chuck Norris `` grenade gag ''\",\n  'Chuck',\n  \"Norris `` grenade gag ''\",\n  'Norris',\n  \"`` grenade gag ''\",\n  '``',\n  \"grenade gag ''\",\n  'grenade',\n  \"gag ''\",\n  'gag',\n  \"''\",\n  'occurs about 7 times during Windtalkers is a good indication of how serious-minded the film is .',\n  'occurs about 7 times during Windtalkers is a good indication of how serious-minded the film is',\n  'occurs',\n  'about 7 times during Windtalkers is a good indication of how serious-minded the film is',\n  'about 7 times during Windtalkers',\n  'about 7 times',\n  '7 times',\n  '7',\n  'times',\n  'during Windtalkers',\n  'during',\n  'Windtalkers',\n  'is a good indication of how serious-minded the film is',\n  'a good indication of how serious-minded the film is',\n  'a good indication',\n  'good indication',\n  'indication',\n  'of how serious-minded the film is',\n  'how serious-minded the film is',\n  'how',\n  'serious-minded the film is',\n  'serious-minded the film',\n  'serious-minded',\n  'the film',\n  'film',\n  'The plot is romantic comedy boilerplate from start to finish .',\n  'The plot',\n  'plot',\n  'is romantic comedy boilerplate from start to finish .',\n  'is romantic comedy boilerplate from start to finish',\n  'romantic comedy boilerplate from start to finish',\n  'romantic comedy boilerplate from start',\n  'romantic comedy boilerplate',\n  'romantic',\n  'comedy boilerplate',\n  'comedy',\n  'boilerplate',\n  'from start',\n  'start',\n  'to finish',\n  'finish',\n  'It arrives with an impeccable pedigree , mongrel pep , and almost indecipherable plot complications .',\n  'arrives with an impeccable pedigree , mongrel pep , and almost indecipherable plot complications .',\n  'arrives with an impeccable pedigree , mongrel pep , and almost indecipherable plot complications',\n  'arrives',\n  'with an impeccable pedigree , mongrel pep , and almost indecipherable plot complications',\n  'an impeccable pedigree , mongrel pep , and almost indecipherable plot complications',\n  'an impeccable pedigree , mongrel pep , and almost',\n  'an impeccable pedigree , mongrel pep , and',\n  'an impeccable pedigree , mongrel pep ,',\n  'an impeccable pedigree , mongrel pep',\n  'impeccable pedigree , mongrel pep',\n  'impeccable',\n  'pedigree , mongrel pep',\n  'pedigree',\n  ', mongrel pep',\n  'mongrel pep',\n  'mongrel',\n  'pep',\n  'almost',\n  'indecipherable plot complications',\n  'indecipherable',\n  'plot complications',\n  'complications',\n  'A film that clearly means to preach exclusively to the converted .',\n  'A film that clearly means to',\n  'A film',\n  'that clearly means to',\n  'clearly means to',\n  'clearly',\n  'means to',\n  'means',\n  'preach exclusively to the converted .',\n  'preach exclusively to the converted',\n  'preach exclusively',\n  'preach',\n  'exclusively',\n  'to the converted',\n  'the converted',\n  'converted',\n  \"While The Importance of Being Earnest offers opportunities for occasional smiles and chuckles , it does n't give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances .\",\n  'While The Importance of Being Earnest offers opportunities for occasional smiles and chuckles',\n  'While',\n  'The Importance of Being Earnest offers opportunities for occasional smiles and chuckles',\n  'The Importance of Being Earnest',\n  'of Being Earnest',\n  'Being Earnest',\n  'offers opportunities for occasional smiles and chuckles',\n  'offers opportunities for occasional smiles and',\n  'offers opportunities for occasional smiles',\n  'offers',\n  'opportunities for occasional smiles',\n  'opportunities',\n  'for occasional smiles',\n  'occasional smiles',\n  'occasional',\n  'smiles',\n  'chuckles',\n  \", it does n't give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances .\",\n  \"it does n't give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances .\",\n  \"does n't give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances .\",\n  \"does n't give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances\",\n  \"give us a reason to be in the theater beyond Wilde 's wit and the actors ' performances\",\n  'give us',\n  'give',\n  \"a reason to be in the theater beyond Wilde 's wit and the actors ' performances\",\n  \"reason to be in the theater beyond Wilde 's wit and the actors ' performances\",\n  \"to be in the theater beyond Wilde 's wit and the actors ' performances\",\n  \"be in the theater beyond Wilde 's wit and the actors ' performances\",\n  'be in the theater',\n  'in the theater',\n  'the theater',\n  'theater',\n  \"beyond Wilde 's wit and the actors ' performances\",\n  'beyond',\n  \"Wilde 's wit and the actors ' performances\",\n  \"Wilde 's wit and\",\n  \"Wilde 's wit\",\n  \"Wilde 's\",\n  'Wilde',\n  \"the actors ' performances\",\n  \"the actors '\",\n  \"actors '\",\n  'actors',\n  \"'\",\n  \"The latest vapid actor 's exercise to appropriate the structure of Arthur Schnitzler 's Reigen .\",\n  'The latest',\n  'latest',\n  \"vapid actor 's exercise to appropriate the structure of Arthur Schnitzler 's Reigen .\",\n  \"vapid actor 's exercise to appropriate the structure of Arthur Schnitzler 's Reigen\",\n  \"vapid actor 's exercise\",\n  'vapid',\n  \"actor 's exercise\",\n  \"actor 's\",\n  'actor',\n  'exercise',\n  \"to appropriate the structure of Arthur Schnitzler 's Reigen\",\n  \"appropriate the structure of Arthur Schnitzler 's Reigen\",\n  'appropriate the structure',\n  'appropriate',\n  'the structure',\n  'structure',\n  \"of Arthur Schnitzler 's Reigen\",\n  \"Arthur Schnitzler 's Reigen\",\n  \"Arthur Schnitzler 's\",\n  'Arthur',\n  \"Schnitzler 's\",\n  'Schnitzler',\n  'Reigen',\n  \"More vaudeville show than well-constructed narrative , but on those terms it 's inoffensive and actually rather sweet .\",\n  \"More vaudeville show than well-constructed narrative , but on those terms it 's inoffensive and actually rather sweet\",\n  'More vaudeville show than well-constructed narrative , but',\n  'More vaudeville show than well-constructed narrative ,',\n  'More vaudeville show than well-constructed narrative',\n  'More vaudeville',\n  'vaudeville',\n  'show than well-constructed narrative',\n  'show',\n  'than well-constructed narrative',\n  'than',\n  'well-constructed narrative',\n  'well-constructed',\n  'narrative',\n  \"on those terms it 's inoffensive and actually rather sweet\",\n  'on those terms',\n  'on',\n  'those terms',\n  'those',\n  'terms',\n  \"it 's inoffensive and actually rather sweet\",\n  \"'s inoffensive and actually rather sweet\",\n  'inoffensive and actually rather sweet',\n  'inoffensive and actually',\n  'inoffensive and',\n  'inoffensive',\n  'actually',\n  'rather sweet',\n  'rather',\n  'Nothing more than a run-of-the-mill action flick .',\n  'more than a run-of-the-mill action flick .',\n  'more than a run-of-the-mill action',\n  'than a run-of-the-mill action',\n  'a run-of-the-mill action',\n  'run-of-the-mill action',\n  'run-of-the-mill',\n  'action',\n  'flick .',\n  'Hampered -- no , paralyzed -- by a self-indulgent script ... that aims for poetry and ends up sounding like satire .',\n  'Hampered -- no , paralyzed -- by a self-indulgent script ... that aims for poetry and ends up sounding like satire',\n  'Hampered -- no , paralyzed -- by a self-indulgent script ...',\n  'Hampered -- no , paralyzed -- by a self-indulgent script',\n  'Hampered -- no , paralyzed --',\n  'Hampered',\n  '-- no , paralyzed --',\n  'no , paralyzed --',\n  'no , paralyzed',\n  'no',\n  ', paralyzed',\n  'paralyzed',\n  'by a self-indulgent script',\n  'a self-indulgent script',\n  'self-indulgent script',\n  'self-indulgent',\n  'script',\n  '...',\n  'that aims for poetry and ends up sounding like satire',\n  'aims for poetry and ends up sounding like satire',\n  'aims for poetry and',\n  'aims for poetry',\n  'aims',\n  'for poetry',\n  'poetry',\n  'ends up sounding like satire',\n  'ends up',\n  'ends',\n  'up',\n  'sounding like satire',\n  'sounding',\n  'like satire',\n  'satire',\n  'Ice Age is the first computer-generated feature cartoon to feel like other movies , and that makes for some glacial pacing early on .',\n  'Ice Age is the first computer-generated feature cartoon to feel like other movies , and that makes for some glacial pacing early on',\n  'Ice Age is the first computer-generated feature cartoon to feel like other movies , and',\n  'Ice Age is the first computer-generated feature cartoon to feel like other movies ,',\n  'Ice Age is the first computer-generated feature cartoon to feel like other movies',\n  'Ice Age',\n  'Ice',\n  'Age',\n  'is the first computer-generated feature cartoon to feel like other movies',\n  'the first computer-generated feature cartoon to feel like other movies',\n  'the first computer-generated feature cartoon',\n  'first computer-generated feature cartoon',\n  'first',\n  'computer-generated feature cartoon',\n  'computer-generated',\n  'feature cartoon',\n  'feature',\n  'cartoon',\n  'to feel like other movies',\n  'feel like other movies',\n  'feel',\n  'like other movies',\n  'other movies',\n  'other',\n  'that makes for some glacial pacing early on',\n  'makes for some glacial pacing early on',\n  'makes for some glacial pacing',\n  'makes',\n  'for some glacial pacing',\n  'some glacial pacing',\n  'glacial pacing',\n  'glacial',\n  'pacing',\n  'early on',\n  'early',\n  \"There 's very little sense to what 's going on here , but the makers serve up the cliches with considerable dash .\",\n  \"There 's very little sense to what 's going on here , but the makers serve up the cliches with considerable dash\",\n  \"There 's very little sense to what 's going on here , but\",\n  \"There 's very little sense to what 's going on here ,\",\n  \"There 's very little sense to what 's going on here\",\n  \"'s very little sense to what 's going on here\",\n  \"very little sense to what 's going on here\",\n  'very little sense',\n  'very little',\n  \"to what 's going on here\",\n  \"what 's going on here\",\n  \"'s going on here\",\n  'going on here',\n  'going on',\n  'here',\n  'the makers serve up the cliches with considerable dash',\n  'the makers',\n  'makers',\n  'serve up the cliches with considerable dash',\n  'serve up',\n  'serve',\n  'the cliches with considerable dash',\n  'the cliches',\n  'cliches',\n  'with considerable dash',\n  'considerable dash',\n  'considerable',\n  'dash',\n  'Cattaneo should have followed the runaway success of his first film , The Full Monty , with something different .',\n  'Cattaneo',\n  'should have followed the runaway success of his first film , The Full Monty , with something different .',\n  'should have followed the runaway success of his first film , The Full Monty , with something different',\n  'have followed the runaway success of his first film , The Full Monty , with something different',\n  'followed the runaway success of his first film , The Full Monty , with something different',\n  'followed the runaway success of his first film , The Full Monty ,',\n  'followed',\n  'the runaway success of his first film , The Full Monty ,',\n  'the runaway success',\n  'runaway success',\n  'runaway',\n  'success',\n  'of his first film , The Full Monty ,',\n  'his first film , The Full Monty ,',\n  'his first film , The Full Monty',\n  'his first film ,',\n  'his first film',\n  'first film',\n  'The Full Monty',\n  'Full Monty',\n  'Full',\n  'Monty',\n  'with something different',\n  'something different',\n  'different',\n  \"They 're the unnamed , easily substitutable forces that serve as whatever terror the heroes of horror movies try to avoid .\",\n  'They',\n  \"'re the unnamed , easily substitutable forces that serve as whatever terror the heroes of horror movies try to avoid .\",\n  \"'re the unnamed , easily substitutable forces that serve as whatever terror the heroes of horror movies try to avoid\",\n  \"'re\",\n  'the unnamed , easily substitutable forces that serve as whatever terror the heroes of horror movies try to avoid',\n  'the unnamed , easily substitutable forces',\n  'unnamed , easily substitutable forces',\n  'unnamed , easily substitutable',\n  'unnamed',\n  ', easily substitutable',\n  'easily substitutable',\n  'easily',\n  'substitutable',\n  'forces',\n  'that serve as whatever terror the heroes of horror movies try to avoid',\n  'serve as whatever terror the heroes of horror movies try to avoid',\n  'as whatever terror the heroes of horror movies try to avoid',\n  'whatever terror the heroes of horror movies try to avoid',\n  'whatever',\n  'terror the heroes of horror movies try to avoid',\n  'terror',\n  'the heroes of horror movies try to avoid',\n  'the heroes of horror movies',\n  'the heroes',\n  'heroes',\n  'of horror movies',\n  'horror movies',\n  'horror',\n  'try to avoid',\n  ...],)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Phrase'].tolist(),"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "with open('movie-xids.npy', 'wb') as f:\n",
    "    np.save(f, tokens['input_ids'])\n",
    "with open('movie-xmask.npy', 'wb') as f:\n",
    "    np.save(f, tokens['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "\n",
    "del tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# first extract sentiment column\n",
    "arr = df['Sentiment'].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 2, 2, ..., 3, 2, 2], dtype=int64)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(156060, 5)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "labels = np.zeros((num_samples, arr.max()+1))\n",
    "labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       ...,\n       [0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "labels[np.arange(num_samples), arr] = 1\n",
    "\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "with open('movie-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Load all the dataset and labels from the files\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "with open('movie-xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f, allow_pickle=True)\n",
    "with open('movie-xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f, allow_pickle=True)\n",
    "with open('movie-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f, allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset element_spec=(TensorSpec(shape=(512,), dtype=tf.int32, name=None), TensorSpec(shape=(512,), dtype=tf.int32, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# insert data as a tuple\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "dataset.take(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# then we use the dataset map method to apply this transformation\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "dataset.take(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None)}, TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset.take(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "8778"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.9\n",
    "# we need to calculate how many batches must be taken to create 90% training set\n",
    "size = int((Xids.shape[0] / batch_size) * split)\n",
    "\n",
    "size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241m.\u001B[39mtake(size)\n\u001B[0;32m      2\u001B[0m val_ds \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mskip(size)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# free up memory\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n",
    "\n",
    "# free up memory\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tzim3\\AppData\\Local\\Temp\\ipykernel_2504\\2648545781.py:1: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.save(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.data.experimental.save(train_ds, 'train')\n",
    "tf.data.experimental.save(val_ds, 'val')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None),\n  'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.int32, name=None)},\n TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.element_spec == train_ds.element_spec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tzim3\\AppData\\Local\\Temp\\ipykernel_2504\\2617767161.py:1: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.experimental.load('train', element_spec=train_ds.element_spec)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "# we can view the model using the summary method\n",
    "bert.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "# convert bert embeddings into 5 output classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            5125        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,102,853\n",
      "Trainable params: 792,581\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# (optional) freeze bert layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# print out model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.float64, name=None)}, TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ({'input_ids': tf.TensorSpec(shape=(16, 512), dtype=tf.float64, name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(16, 512), dtype=tf.float64, name=None)},\n",
    "                tf.TensorSpec(shape=(16, 5), dtype=tf.float64, name=None))\n",
    "\n",
    "# load the training and validation sets\n",
    "train_ds = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "val_ds = tf.data.experimental.load('val', element_spec=element_spec)\n",
    "\n",
    "# view the input format\n",
    "train_ds.take(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tzim3\\AppData\\Local\\Temp\\ipykernel_2504\\1377320828.py\", line 1, in <module>\n      history = model.fit(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 753, in run_call_with_unpacked_inputs\n      mismatched_keys.update(mismatched_keys_set)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 323, in call\n      attention_probs = self.dropout(inputs=attention_probs, training=training)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[16,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_36609]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\n\u001B[0;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Alpha\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tzim3\\AppData\\Local\\Temp\\ipykernel_2504\\1377320828.py\", line 1, in <module>\n      history = model.fit(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 753, in run_call_with_unpacked_inputs\n      mismatched_keys.update(mismatched_keys_set)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 323, in call\n      attention_probs = self.dropout(inputs=attention_probs, training=training)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\tzim3\\anaconda3\\envs\\Alpha\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[16,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node model_2/bert/encoder/layer_._9/attention/self/dropout_65/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_36609]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('sentiment_model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOAD AND PREDICT SECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('sentiment_model')\n",
    "# view model architecture to confirm we have save and loaded correctly\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def prep_data(text):\n",
    "    tokens = tokenizer.encode_plus(text, max_length=512,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    # tokenizer returns int32 tensors, we need to return float64, so we use tf.cast\n",
    "    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n",
    "            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probs = model.predict(prep_data(\"hello world\"))[0]\n",
    "\n",
    "probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(probs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# so we can see full phrase\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv('test.tsv', sep='\\t')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['SentenceId'], keep='first')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Sentiment'] = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # get token tensors\n",
    "    tokens = prep_data(row['Phrase'])\n",
    "    # get probabilities\n",
    "    probs = model.predict(tokens)\n",
    "    # find argmax for winning class\n",
    "    pred = np.argmax(probs)\n",
    "    # add to dataframe\n",
    "    df.at[i, 'Sentiment'] = pred\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.tail()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
