{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "STOPWORDS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i’m', 'amazed', 'how', 'often', 'in', 'practice,', 'not', 'only', 'does', 'a', '@huggingface', 'nlp', 'model', 'solve', 'your', 'problem,', 'but', 'one', 'of', 'their', 'public', 'finetuned', 'checkpoints,', 'is', 'good', 'enough', 'for', 'the', 'job.', 'both', 'impressed,', 'and', 'a', 'little', 'disappointed', 'how', 'rarely', 'i', 'get', 'to', 'actually', 'train', 'a', 'model', 'that', 'matters', ':(']\n",
      "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
      "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\"\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words\n",
    "tweet = tweet.lower().split()\n",
    "print(tweet)\n",
    "\n",
    "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "print(\"With stopwords:\", ' '.join(tweet))\n",
    "print(\"Without:\", ' '.join(tweet_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter | Lancaster\n",
      "happi | happy\n",
      "happiest | happiest\n",
      "happier | happy\n",
      "cactu | cact\n",
      "cactii | cacti\n",
      "eleph | eleph\n",
      "eleph | eleph\n",
      "amaz | amaz\n",
      "amaz | amaz\n",
      "amazingli | amaz\n",
      "cement | cem\n",
      "owe | ow\n",
      "maximum | maxim\n"
     ]
    }
   ],
   "source": [
    "txt = \"I am amazed by how amazingly amazing you are\"\n",
    "\n",
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# MORE LIGHTWEIGHT\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# mORE ROBUST\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed = [(porter.stem(word), lancaster.stem(word)) for word in words_to_stem]\n",
    "\n",
    "print(\"Porter | Lancaster\")\n",
    "for stem in stemmed:\n",
    "    print(f\"{stem[0]} | {stem[1]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LEMMATIZATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tzim3\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "['amaze', 'amazed', 'amazing']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['amaze', 'amazed', 'amazing']\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in words]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['amaze', 'amaze', 'amaze']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "[lemmatizer.lemmatize(word, wordnet.VERB) for word in words]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UniCode normalization -Canonical compatibilty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ουσιαστικά compose kai decompose\n",
    "\n",
    "Name\tAbbreviation\tDescription\tExample\n",
    "Form D\tNFD\tCanonical decomposition\tÇ → C ̧\n",
    "Form C\tNFC\tCanoncial decomposition followed by canonical composition\tÇ → C ̧ → Ç\n",
    "Form KD\tNFKD\tCompatibility decomposition\tℌ ̧ → H ̧\n",
    "Form KC\tNFKC\tCompatibility decomposition followed by canonical composition\tℌ ̧ → H ̧ → Ḩ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'H'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "c_with_cedilla = \"\\u00C7\"  # Latin capital C with cedilla (single character)\n",
    "c_with_cedilla\n",
    "\n",
    "c_plus_cedilla = \"\\u0043\\u0327\"  # \\u0043 = Latin capital C, \\u0327 = 'combining cedilla' (two characters)\n",
    "c_plus_cedilla\n",
    "\n",
    "unicodedata.normalize('NFD', c_with_cedilla) == c_plus_cedilla\n",
    "unicodedata.normalize('NFC', c_with_cedilla) == c_plus_cedilla\n",
    "\n",
    "unicodedata.normalize('NFKD', 'ℌ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
